{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f091e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n",
      "Torchvision version: 0.14.0\n",
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from monai.metrics import DiceMetric, MeanIoU, SurfaceDiceMetric, SSIMMetric, GeneralizedDiceScore\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.losses import GeneralizedDiceLoss, DiceLoss, GeneralizedDiceFocalLoss\n",
    "from monai.metrics import DiceMetric, GeneralizedDiceScore\n",
    "from LinearWarmupCosine import LinearWarmupCosineAnnealingLR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fc5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Yuheng Li\\Segment Anything\\TestDataset\\TestDataset\\ETIS-LaribPolypDB\\images\\10.png D:\\Yuheng Li\\Segment Anything\\TestDataset\\TestDataset\\ETIS-LaribPolypDB\\masks\\10.png 196 196\n"
     ]
    }
   ],
   "source": [
    "kv_folder = 'D:\\Yuheng Li\\Segment Anything\\\\TestDataset\\\\TestDataset\\\\Kvasir'\n",
    "cvc_db_folder =  'D:\\Yuheng Li\\Segment Anything\\\\TestDataset\\\\TestDataset\\\\CVC-ClinicDB'\n",
    "cvc_colon_folder = 'D:\\Yuheng Li\\Segment Anything\\\\TestDataset\\\\TestDataset\\\\CVC-ColonDB'\n",
    "cvc_300_folder = 'D:\\Yuheng Li\\Segment Anything\\\\TestDataset\\\\TestDataset\\\\CVC-300'\n",
    "etis_folder = 'D:\\Yuheng Li\\Segment Anything\\\\TestDataset\\\\TestDataset\\\\ETIS-LaribPolypDB'\n",
    "\n",
    "\n",
    "image_path = []\n",
    "mask_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(etis_folder, topdown=False): #finds MRI files\n",
    "    for name in files:\n",
    "        if name.endswith(\".png\"):\n",
    "            apath=os.path.join(root, name)\n",
    "            if 'images' in apath:\n",
    "                image_path.append(apath)\n",
    "            if 'masks' in apath:\n",
    "                mask_path.append(apath)\n",
    "                \n",
    "print(image_path[1],mask_path[1], len(image_path), len(mask_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596f3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "# model_type = \"vit_h\"\n",
    "\n",
    "# sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "# model_type = \"vit_b\"\n",
    "\n",
    "sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "model_type = \"vit_l\"\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "transform = ResizeLongestSide(sam.image_encoder.img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d7da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bboxes(mask, num_instances):\n",
    "\n",
    "    \"\"\"Compute bounding boxes from masks.\n",
    "\n",
    "    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
    "\n",
    " \n",
    "\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = np.zeros([num_instances, 4], dtype=np.int32)\n",
    "\n",
    "    for i in range(num_instances):\n",
    "\n",
    "        m = mask\n",
    "\n",
    "        # Bounding box.\n",
    "\n",
    "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
    "\n",
    "#         print(\"np.any(m, axis=0)\",np.any(m, axis=0))\n",
    "\n",
    "#         print(\"p.where(np.any(m, axis=0))\",np.where(np.any(m, axis=0)))\n",
    "\n",
    "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
    "\n",
    "        if horizontal_indicies.shape[0]:\n",
    "\n",
    "            x1, x2 = horizontal_indicies[[0, -1]]\n",
    "\n",
    "            y1, y2 = vertical_indicies[[0, -1]]\n",
    "\n",
    "            # x2 and y2 should not be part of the box. Increment by 1.\n",
    "\n",
    "            x2 += 1\n",
    "\n",
    "            y2 += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # No mask for this instance. Might happen due to\n",
    "\n",
    "            # resizing or cropping. Set bbox to zeros\n",
    "\n",
    "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "\n",
    "        boxes[i] = np.array([y1, x1, y2, x2])\n",
    "\n",
    "    return boxes.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2911dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco mask style dataloader\n",
    "\n",
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, image_size):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # TODO: use ResizeLongestSide and pad to square\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.image_path[index].split('images\\\\')[1].split('.png')[0]\n",
    "\n",
    "        image = cv2.imread(self.image_path[index])\n",
    "        gt = cv2.imread(self.mask_path[index])\n",
    "        gt = cv2.cvtColor(gt, cv2.COLOR_BGR2GRAY) / 255\n",
    "        gt = gt.astype('float32')\n",
    "\n",
    "        bbox_arr = extract_bboxes(gt, 1)\n",
    "\n",
    "        gt_resized = cv2.resize(gt, (1024, 1024), cv2.INTER_NEAREST)\n",
    "        gt_resized = torch.as_tensor(gt_resized > 0).long()\n",
    "        \n",
    "        gt = torch.from_numpy(gt)\n",
    "        gt_binary_mask = torch.as_tensor(gt > 0).long()\n",
    "\n",
    "        transform = ResizeLongestSide(self.image_size)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image =  cv2.resize(input_image, (1024, 1024), cv2.INTER_CUBIC)\n",
    "        input_image= self.to_tensor(input_image)\n",
    "        \n",
    "        # input_image= self.normalize(input_image)\n",
    "#         print(input_image.shape)\n",
    "#         plt.figure()\n",
    "#         plt.imshow(input_image[0])\n",
    "#         print('before preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))\n",
    "        # input_image = sam.preprocess(input_image.to('cuda:0')).detach().cpu()\n",
    "#         print('after preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))\n",
    "#         input_image = cv2.resize(input_image.numpy(), (1024, 1024), cv2.INTER_CUBIC)\n",
    "\n",
    "#         plt.figure()\n",
    "#         plt.imshow(input_image[0])\n",
    "        \n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(input_image.shape[-2:])\n",
    "        \n",
    "        return input_image, np.array(bbox_arr), gt_binary_mask, gt_resized, original_image_size, input_size\n",
    "    \n",
    "\n",
    "def my_collate(batch):\n",
    "    \n",
    "    images, bboxes, masks, gt_resized, original_image_size, input_size = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    gt_resized = torch.stack(gt_resized, dim=0)\n",
    "    \n",
    "    masks = [m for m in masks]\n",
    "    bboxes = [m for m in bboxes]\n",
    "    original_image_size = [m for m in original_image_size]\n",
    "    input_size = [m for m in input_size]\n",
    "    \n",
    "    return images, bboxes, masks, gt_resized, original_image_size, input_size\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38e5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = ColonDataset(image_path, mask_path, sam.image_encoder.img_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn = my_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eeb8563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean val dice: 0.9054498590376913\n",
      "Mean val gd: 0.6419873968698084\n",
      "Mean val iou: 0.8603473611328066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model_path =  'D:\\\\Yuheng Li\\\\Segment Anything\\\\Model results\\\\CVC Clinical\\\\SAM Finetune Enc Dec'\n",
    "\n",
    "# model_path  = 'D:\\\\Yuheng Li\\\\Segment Anything\\\\Model results\\\\All datasets\\\\SAM Finetune Enc Dec'\n",
    "\n",
    "# model_path = 'D:\\\\Yuheng Li\\\\Segment Anything\\\\Model results\\\\All datasets\\\\SAM Finetune Enc Dec'\n",
    "\n",
    "model_path = 'D:\\\\Yuheng Li\\\\Segment Anything\\\\Model results\\\\All datasets\\\\SAM_L Finetune Enc Dec\\\\SAM Finetune Enc Dec'\n",
    "\n",
    "\n",
    "sam.prompt_encoder.load_state_dict(torch.load(os.path.join(model_path, \"prompt_enc_best_dice_model_DL.pth\")))\n",
    "sam.image_encoder.load_state_dict(torch.load(os.path.join(model_path, \"img_enc_best_dice_model_DL.pth\")))\n",
    "sam.mask_decoder.load_state_dict(torch.load(os.path.join(model_path, \"dec_best_dice_model_DL.pth\")))\n",
    "sam.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_dice = []\n",
    "    batch_gd = []\n",
    "    batch_iou = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]\n",
    "\n",
    "        dice = DiceMetric()\n",
    "        gd =  GeneralizedDiceScore()\n",
    "        iou = MeanIoU()\n",
    "\n",
    "        for i in range(len(mask)):\n",
    "            image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))\n",
    "\n",
    "            orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]\n",
    "            col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y\n",
    "            col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x\n",
    "\n",
    "            box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()\n",
    "\n",
    "            num_masks = box.shape[0]\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "              points=None,\n",
    "              boxes= box_torch,\n",
    "              masks = None\n",
    "            )\n",
    "\n",
    "            low_res_masks, iou_predictions = sam.mask_decoder(\n",
    "              image_embeddings=image_embedding,\n",
    "              image_pe=sam.prompt_encoder.get_dense_pe(),\n",
    "              sparse_prompt_embeddings=sparse_embeddings,\n",
    "              dense_prompt_embeddings=dense_embeddings,\n",
    "              multimask_output=False\n",
    "            )\n",
    "\n",
    "            upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i])\n",
    "\n",
    "            binary_mask = torch.sigmoid(upscaled_masks.detach().cpu())\n",
    "            binary_mask =  (binary_mask>0.5).float()\n",
    "\n",
    "            gt_binary_mask = mask[i].detach().cpu()\n",
    "\n",
    "            if binary_mask.size()[0] > 1:\n",
    "                binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)\n",
    "\n",
    "            dice.reset()\n",
    "            gd.reset()\n",
    "            iou.reset()\n",
    "\n",
    "            dice(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            gd(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            iou(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            final_dice = dice.aggregate().numpy()[0]\n",
    "            final_gd = gd.aggregate().numpy()[0]\n",
    "            final_iou = iou.aggregate().numpy()[0]\n",
    "            batch_dice.append(final_dice)\n",
    "            batch_gd.append(final_gd)\n",
    "            batch_iou.append(final_iou)\n",
    "\n",
    "\n",
    "    print(f'Mean val dice: {sum(batch_dice) / len(batch_dice)}')\n",
    "    print(f'Mean val gd: {sum(batch_gd) / len(batch_gd)}')\n",
    "    print(f'Mean val iou: {sum(batch_iou) / len(batch_iou)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf80eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset\n",
    "         ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
